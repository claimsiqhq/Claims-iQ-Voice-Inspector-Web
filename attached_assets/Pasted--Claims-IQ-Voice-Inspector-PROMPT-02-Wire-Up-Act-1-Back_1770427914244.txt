# Claims IQ Voice Inspector — PROMPT 02: Wire Up Act 1 Backend

## Current State of the Codebase

The frontend UI is fully built from the UX Design Spec. All pages and components exist. What's missing is the backend logic — the database schema, API routes, OpenAI integration, and wiring the existing UI to real data.

### Existing Tech Stack (DO NOT CHANGE)
- **Frontend:** React 19, Vite 7, TypeScript, Tailwind CSS v4, shadcn/ui (full Radix), wouter routing, TanStack React Query, Framer Motion, Recharts
- **Backend:** Express 5, WebSocket (ws)
- **Database:** Drizzle ORM + PostgreSQL
- **Package manager:** npm

### Existing File Structure
```
client/
  src/
    pages/
      ClaimsList.tsx          ← Screen 1 (built)
      DocumentUpload.tsx      ← Screen 2 (built)
      ExtractionReview.tsx    ← Screen 3 (built)
      InspectionBriefing.tsx  ← Screen 4 (built)
      ActiveInspection.tsx    ← Screen 5 (built)
      not-found.tsx
    components/
      Layout.tsx              ← Nav + layout shell (built)
      ClaimCard.tsx           ← Claim card component (built)
      VoiceIndicator.tsx      ← Voice state indicator (built)
      StatusBadge.tsx         ← Status badges (built)
      ui/                     ← shadcn/ui components
    hooks/
      use-mobile.tsx
      use-toast.ts
    lib/
      queryClient.ts
      utils.ts
    App.tsx                   ← Routing (built)
    index.css                 ← Claims IQ brand tokens (built)
    main.tsx
server/
  index.ts                    ← Server entry
  routes.ts                   ← API routes (EMPTY — needs implementation)
  storage.ts                  ← Storage interface (EMPTY — needs implementation)
  static.ts
  vite.ts
shared/
  schema.ts                   ← DB schema (MINIMAL — needs expansion)
```

### What This Prompt Does

Add the complete Act 1 backend: database schema, API routes, OpenAI document parsing, and connect the existing frontend pages to real data. After this prompt, Screens 1-4 should be fully functional with real document upload, AI extraction, and briefing generation.

---

## Step 1: Expand the Database Schema

**File: `shared/schema.ts`**

Replace the current minimal schema with the full Act 1 data model using Drizzle ORM. Keep any existing table definitions but ADD these tables:

```typescript
import { pgTable, text, serial, integer, boolean, timestamp, jsonb, varchar, real } from "drizzle-orm/pg-core";
import { createInsertSchema } from "drizzle-zod";
import { z } from "zod";

// ── Claims ───────────────────────────────────────────
export const claims = pgTable("claims", {
  id: serial("id").primaryKey(),
  claimNumber: varchar("claim_number", { length: 50 }).notNull(),
  insuredName: text("insured_name"),
  propertyAddress: text("property_address"),
  city: varchar("city", { length: 100 }),
  state: varchar("state", { length: 2 }),
  zip: varchar("zip", { length: 10 }),
  dateOfLoss: varchar("date_of_loss", { length: 20 }),
  perilType: varchar("peril_type", { length: 20 }),  // hail, wind, water, fire, freeze, multi
  status: varchar("status", { length: 30 }).notNull().default("draft"),
  // Status values: draft, documents_uploaded, extractions_confirmed, briefing_ready, inspecting, review, complete
  createdAt: timestamp("created_at").defaultNow(),
  updatedAt: timestamp("updated_at").defaultNow(),
});

// ── Documents (uploaded PDFs) ────────────────────────
export const documents = pgTable("documents", {
  id: serial("id").primaryKey(),
  claimId: integer("claim_id").notNull().references(() => claims.id),
  documentType: varchar("document_type", { length: 20 }).notNull(), // fnol, policy, endorsements
  fileName: text("file_name"),
  fileSize: integer("file_size"),
  filePath: text("file_path"),       // local storage path
  rawText: text("raw_text"),         // extracted PDF text
  status: varchar("status", { length: 20 }).notNull().default("empty"),
  // Status values: empty, uploading, uploaded, processing, parsed, error
  errorMessage: text("error_message"),
  createdAt: timestamp("created_at").defaultNow(),
});

// ── Extractions (AI-parsed structured data) ──────────
export const extractions = pgTable("extractions", {
  id: serial("id").primaryKey(),
  claimId: integer("claim_id").notNull().references(() => claims.id),
  documentType: varchar("document_type", { length: 20 }).notNull(), // fnol, policy, endorsements
  extractedData: jsonb("extracted_data").notNull(),   // the full structured extraction JSON
  confidence: jsonb("confidence"),                     // per-field confidence scores
  confirmedByUser: boolean("confirmed_by_user").default(false),
  createdAt: timestamp("created_at").defaultNow(),
  updatedAt: timestamp("updated_at").defaultNow(),
});

// ── Briefings (synthesized from all 3 extractions) ───
export const briefings = pgTable("briefings", {
  id: serial("id").primaryKey(),
  claimId: integer("claim_id").notNull().references(() => claims.id),
  propertyProfile: jsonb("property_profile"),
  coverageSnapshot: jsonb("coverage_snapshot"),
  perilAnalysis: jsonb("peril_analysis"),
  endorsementImpacts: jsonb("endorsement_impacts"),
  inspectionChecklist: jsonb("inspection_checklist"),
  dutiesAfterLoss: jsonb("duties_after_loss"),
  redFlags: jsonb("red_flags"),
  createdAt: timestamp("created_at").defaultNow(),
});

// Export insert schemas for validation
export const insertClaimSchema = createInsertSchema(claims);
export const insertDocumentSchema = createInsertSchema(documents);
export const insertExtractionSchema = createInsertSchema(extractions);
export const insertBriefingSchema = createInsertSchema(briefings);
```

After updating the schema, run `npm run db:push` to push changes to PostgreSQL.

---

## Step 2: Implement Storage Layer

**File: `server/storage.ts`**

Replace the current minimal storage with a full implementation using Drizzle queries for all CRUD operations:

```typescript
// Required operations:

// Claims
createClaim(data) → claim
getClaims() → claim[]
getClaim(id) → claim
updateClaimStatus(id, status) → claim

// Documents
createDocument(claimId, documentType, fileName, filePath, fileSize) → document
getDocuments(claimId) → document[]
getDocument(claimId, documentType) → document
updateDocumentStatus(id, status, rawText?) → document
updateDocumentError(id, errorMessage) → document

// Extractions
createExtraction(claimId, documentType, extractedData, confidence) → extraction
getExtractions(claimId) → extraction[]
getExtraction(claimId, documentType) → extraction
updateExtraction(id, extractedData) → extraction
confirmExtraction(id) → extraction

// Briefings
createBriefing(claimId, briefingData) → briefing
getBriefing(claimId) → briefing
```

---

## Step 3: Implement API Routes

**File: `server/routes.ts`**

Add these REST endpoints to the existing Express router:

### Claims CRUD
```
GET    /api/claims              → List all claims
POST   /api/claims              → Create new claim { claimNumber, insuredName?, propertyAddress? }
GET    /api/claims/:id          → Get single claim with documents, extractions
PATCH  /api/claims/:id          → Update claim fields
```

### Document Upload & Parsing
```
POST   /api/claims/:id/documents/upload
  - Content-Type: multipart/form-data
  - Fields: file (PDF), documentType (fnol | policy | endorsements)
  - Action:
    1. Save file to disk (use /tmp/claims/:id/ or a configurable uploads directory)
    2. Create document record with status "uploaded"
    3. Return { documentId, status: "uploaded" }

POST   /api/claims/:id/documents/:type/parse
  - Action:
    1. Read the uploaded file for this claim + type
    2. Extract text from PDF using pdf-parse (npm install pdf-parse)
    3. Send extracted text to OpenAI gpt-4o with the appropriate extraction prompt (see Section 4)
    4. Parse the JSON response
    5. Create extraction record with extracted data + confidence scores
    6. Update document status to "parsed"
    7. If all 3 documents are parsed, update claim status to "documents_uploaded"
    8. Return { extraction, confidence }

GET    /api/claims/:id/documents
  - Returns all documents for this claim with their status
```

### Extractions Review
```
GET    /api/claims/:id/extractions
  - Returns all extractions for this claim (fnol, policy, endorsements)

GET    /api/claims/:id/extractions/:type
  - Returns single extraction (fnol | policy | endorsements)

PUT    /api/claims/:id/extractions/:type
  - Body: { extractedData: { ...corrected fields } }
  - Updates the extraction with adjuster's corrections
  - Sets confirmedByUser = true

POST   /api/claims/:id/extractions/confirm-all
  - Marks all 3 extractions as confirmed
  - Updates claim status to "extractions_confirmed"
```

### Briefing Generation
```
POST   /api/claims/:id/briefing/generate
  - Action:
    1. Load all 3 confirmed extractions for this claim
    2. Send to OpenAI gpt-4o with the briefing synthesis prompt (see Section 4)
    3. Parse the structured briefing JSON response
    4. Create briefing record
    5. Update claim status to "briefing_ready"
    6. Return the full briefing object

GET    /api/claims/:id/briefing
  - Returns the generated briefing for this claim
```

### Install required packages:
```bash
npm install pdf-parse openai multer @types/multer
```

Use `multer` for handling multipart file uploads in Express.

---

## Step 4: OpenAI Integration

Create a new file: **`server/openai.ts`**

This module handles all OpenAI API calls for document parsing and briefing generation.

```typescript
import OpenAI from "openai";

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
```

### FNOL Extraction Function
Call `gpt-4o` with this system prompt:
```
You are a claims document parser for an insurance inspection platform.
Extract structured data from this First Notice of Loss (FNOL) document.

Return a JSON object with these fields:
{
  "claimNumber": string,
  "insuredName": string,
  "propertyAddress": { "street": string, "city": string, "state": string, "zip": string },
  "dateOfLoss": string (ISO date),
  "perilType": "hail" | "wind" | "water" | "fire" | "freeze" | "multi",
  "reportedDamage": string,
  "propertyType": "single_family" | "townhouse" | "condo" | "multi_family",
  "yearBuilt": number | null,
  "stories": number | null,
  "squareFootage": number | null,
  "confidence": {
    [field]: "high" | "medium" | "low"  // for each field above
  }
}
If a field cannot be determined, set to null with confidence "low".
Return ONLY valid JSON.
```

### Policy Extraction Function (HO 80 03)
Call `gpt-4o` with this system prompt:
```
You are a claims document parser for an insurance inspection platform.
Extract structured data from this Homeowner Insurance Policy (HO 80 03 or similar).

Return a JSON object with these fields:
{
  "policyNumber": string,
  "policyType": string,
  "coverageA": number,
  "coverageB": number,
  "coverageC": number,
  "coverageD": number,
  "coverageE": number | null,
  "coverageF": number | null,
  "deductible": { "amount": number, "type": "flat" | "percentage" | "wind_hail_specific", "windHailDeductible": number | null },
  "lossSettlement": "replacement_cost" | "actual_cash_value" | "functional_replacement",
  "constructionType": string,
  "roofType": string | null,
  "yearBuilt": number | null,
  "specialConditions": string[] | null,
  "confidence": { [field]: "high" | "medium" | "low" }
}
Return ONLY valid JSON.
```

### Endorsements Extraction Function
Call `gpt-4o` with this system prompt:
```
You are a claims document parser for an insurance inspection platform.
Extract all endorsements from this insurance policy endorsements document.

Return a JSON object:
{
  "endorsements": [
    {
      "endorsementId": string (e.g., "HO 88 02"),
      "title": string,
      "whatItModifies": string,
      "effectiveDate": string | null,
      "keyProvisions": string[],
      "sublimits": [{ "description": string, "amount": number }] | null,
      "claimImpact": string
    }
  ],
  "totalEndorsements": number,
  "confidence": "high" | "medium" | "low"
}

Common endorsements: HO 88 02 (Roof Surfaces), HO 81 17 (Water Back-Up), HO 86 05 (Ordinance/Law), HO 82 33 (Mine Subsidence), HO 84 19 (Personal Property RCV).
Return ONLY valid JSON.
```

### Briefing Generation Function
Call `gpt-4o` with all 3 confirmed extractions as context:
```
You are an expert insurance claims analyst preparing an inspection briefing for a field adjuster.
Synthesize the FNOL, Policy, and Endorsements data into a comprehensive pre-inspection briefing.

Return a JSON object:
{
  "propertyProfile": {
    "address": string, "propertyType": string, "yearBuilt": number,
    "stories": number, "constructionType": string, "roofType": string,
    "squareFootage": number | null, "summary": string
  },
  "coverageSnapshot": {
    "coverageA": { "label": "Dwelling", "limit": number },
    "coverageB": { "label": "Other Structures", "limit": number },
    "coverageC": { "label": "Personal Property", "limit": number },
    "coverageD": { "label": "Loss of Use", "limit": number },
    "deductible": number, "deductibleType": string,
    "lossSettlement": string, "summary": string
  },
  "perilAnalysis": {
    "perilType": string,
    "whatToLookFor": string[],
    "inspectionPriorities": string[],
    "typicalDamagePatterns": string,
    "commonMistakes": string[]
  },
  "endorsementImpacts": [
    { "endorsementId": string, "title": string, "adjusterGuidance": string }
  ],
  "inspectionChecklist": {
    "exterior": string[], "roof": string[],
    "interior": string[], "systems": string[],
    "documentation": string[]
  },
  "dutiesAfterLoss": string[],
  "redFlags": string[]
}
Return ONLY valid JSON.
```

User message format for briefing:
```
Generate an inspection briefing from this claim data:

FNOL: {fnolExtraction JSON}
Policy: {policyExtraction JSON}
Endorsements: {endorsementsExtraction JSON}
```

---

## Step 5: Wire Frontend to Backend

The page components already exist. They need to be connected to the new API endpoints using TanStack React Query (already installed).

### ClaimsList.tsx
- `useQuery` to fetch `GET /api/claims`
- Replace any hardcoded sample data with query results
- "New Claim" button calls `useMutation` on `POST /api/claims`
- Clicking a claim navigates to its current step

### DocumentUpload.tsx
- File input or drag-drop triggers `POST /api/claims/:id/documents/upload` via `useMutation`
- After upload succeeds, auto-call `POST /api/claims/:id/documents/:type/parse`
- Poll or use the mutation response to update card states (uploading → processing → parsed)
- "Generate Briefing" button navigates to ExtractionReview when all 3 are parsed

### ExtractionReview.tsx
- `useQuery` to fetch `GET /api/claims/:id/extractions`
- Populate the three tabs with real extraction data
- Editable fields call `PUT /api/claims/:id/extractions/:type` on save
- Confidence indicators (green/amber/red) driven by the confidence field
- "Confirm & Generate Briefing" calls `POST /api/claims/:id/extractions/confirm-all` then `POST /api/claims/:id/briefing/generate`, then navigates to InspectionBriefing

### InspectionBriefing.tsx
- `useQuery` to fetch `GET /api/claims/:id/briefing`
- Populate all sections (Property Profile, Coverage Snapshot, Peril Analysis, Endorsement Impacts, Checklist, Duties)
- "START INSPECTION" button navigates to ActiveInspection

### ActiveInspection.tsx
- No changes needed for this prompt — voice integration comes in PROMPT-03
- Can show a toast: "Voice agent will be connected in the next update"

---

## Step 6: Seed Data

Create a seed script or add to the server startup to create the Gold Standard sample claim if no claims exist:

```
Claim Number: CLM-2024-00847
Insured: Robert & Sarah Penson
Property: 1847 Maple Ridge Drive, Sullivan, IN 47882
Date of Loss: March 14, 2024
Peril: Hail
Status: draft
```

This gives the adjuster something to click on immediately when they open the app.

---

## Environment Variable

```env
OPENAI_API_KEY=sk-...
```

This is the only new environment variable needed. The PostgreSQL DATABASE_URL should already be configured from the Replit setup.

---

## Test Flow After Implementation

1. Open app → see Claims List with seeded Penson claim
2. Click claim → navigate to Document Upload
3. Upload a FNOL PDF → card transitions through uploading → processing → parsed with extraction preview
4. Upload Policy PDF → same flow
5. Upload Endorsements PDF → same flow, "Generate Briefing" enables
6. Click through to Extraction Review → see AI-parsed fields in editable forms
7. Amber highlights on low-confidence fields
8. Edit a field, confirm extractions
9. Briefing generates → see Property Profile, Coverage Snapshot, Peril Analysis, Endorsement Impacts, Checklist
10. Click "START INSPECTION" → lands on the voice HUD (no voice yet)

---

## What NOT to Change

- Do NOT alter any existing page layouts, components, or styling
- Do NOT change the routing in App.tsx
- Do NOT remove or restructure existing client code
- Do NOT add authentication (POC)
- ONLY add backend logic and connect it to the existing frontend
